from transformers import GPT2Tokenizer, GPT2LMHeadModel
from stable_baselines3 import PPO, A2C
from models.GPT2ActorCriticPolicy import GPT2ActorCriticPolicy
from envs.AdaptEnv import AdaptEnv
import os

# device = 'cpu'
device = 'cuda:0'

if device == 'cpu':
    print('WARNING: training on cpu.')

cache_dir = '/om2/user/rogerjin/.cache/transformers'

import os
os.environ['TRANSFORMERS_CACHE'] = cache_dir
os.environ['WANDB_CACHE_DIR'] = cache_dir

hf_models = {
    'gpt2': 'gpt2',
    'gpt2-medium': 'gpt2-medium',
    'gpt2-large': 'gpt2-large',
    'DialoGPT-small': 'microsoft/DialoGPT-small',
    'DialoGPT-medium': 'microsoft/DialoGPT-medium',
    'DialoGPT-large': 'microsoft/DialoGPT-large'
}

learning_algos = {
    'ppo': PPO,
    'a2c': A2C
}

import argparse
import json

parser = argparse.ArgumentParser()
parser.add_argument('--config')
parser.add_argument('--ckpt')
args = parser.parse_args()
config_path = args.config
checkpoint_dir = args.ckpt

config = json.loads(open(config_path).read())
config.setdefault('temperature', 0.6)

print(config)

TOY = False
# TOY = True
config['toy'] = TOY
if TOY:
    print('TOY')
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    config['max_steps'] = 4
    config['max_seq_len'] = 10
    config['algorithm'] = 'a2c'
    config['n_envs'] = 2
    config['save_freq'] = 2
    config['gen_freq'] = 1
    config['model_name'] = 'gpt2'

config['policy_kwargs'] = {
    'ortho_init': False, # important - use pretrained weights
    'policy_hfid': hf_models[config['model_name']],
    'value_hfid': hf_models[config['model_name']],
    'device': device,
    'cache_dir': cache_dir,
}

today = datetime.today().strftime('%Y-%m-%d')
curr_time = datetime.now().strftime('%H-%M-%S')

save_base_dir='checkpoints'
save_rel_dir=f"{config['out_type']}/{config['emotion']}/{config['model_name']}/{today}/{curr_time}"
save_dir = f'{save_base_dir}/{save_rel_dir}' if not TOY else 'test/checkpoints'

pprint.pprint(config, indent=2)




prompt = 'Tell me a story. '

out_dir = f'output/happy/story/DialoGPT-small/2022-05-06/15-27-24/'
os.makedirs(out_dir, exist_ok=True)
with open(f'{out_dir}/prompt.txt', 'w+') as prompt_file:
    prompt_file.write(prompt)

cache_dir = '/om2/user/rogerjin/.cache/transformers'
hf_id = 'microsoft/DialoGPT-small'
# hf_id = 'gpt2-large'
device = 'cuda:0'
save_path = '/om2/user/rogerjin/6.884/adaPT/checkpoints/happy/story/DialoGPT-small/2022-05-06/15-27-24/DialoGPT-small_ppo_10000_lr=5e-06.zip'
tokenizer = GPT2Tokenizer.from_pretrained(hf_id, cache_dir=cache_dir, device=device)

control = False
run_name = 'be_happier'

if control:
    model = GPT2LMHeadModel.from_pretrained(hf_id, cache_dir=cache_dir).to(device)
    out_path = f'{out_dir}/control.txt'
else:
    ppo = PPO.load(save_path)
    model = ppo.policy._modules['mlp_extractor'].policy_latent.lm # todo: figure out if the weights are different
    out_path = f'{out_dir}/{run_name}_t=3.txt'
# when generating, we will use the logits of right-most token to predict the next token
# so the padding should be on the left
tokenizer.padding_side = "left" 
tokenizer.pad_token = tokenizer.eos_token # to avoid an error

num_sequences = 100
sentences = [prompt] * num_sequences
batch_size = 1

output_sequences = []

for i in range(0, num_sequences, batch_size):
    batch = sentences[i:i+batch_size]
    inputs = tokenizer(batch, return_tensors="pt", padding=True)

    output_sequences.extend(model.generate(
        input_ids=inputs['input_ids'].to(device),
        attention_mask=inputs['attention_mask'].to(device),
        do_sample=True, # disable sampling to test if batching affects output
        min_length=25,
        max_length = 60,
        temperature=0.6,
        top_k=20,
        no_repeat_ngram_size=2
    ))

decodings = []

for i in range(len(sentences)):
    decoding = tokenizer.decode(output_sequences[i], skip_special_tokens=True)
    print(decoding)
    decodings.append(f'<|DECODING|>{decoding}')
#     decodings.append(f'{decoding}\n')
    # you can use skip_special_tokens=True in decode() to remove padding token
    # but note that it will also remove other special_tokens

with open(out_path, 'w+') as out:
    out.writelines(decodings)
