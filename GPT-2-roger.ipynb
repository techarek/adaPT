{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e868e81-4e21-4b2b-b374-d2134caa77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4303b-1c5f-44f3-b39f-f7bb0eeadba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55b1a2-65f1-4563-96b4-613721e83f76",
   "metadata": {},
   "source": [
    "## Test out Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd40cb0-fc97-421d-b3d3-2d1eacb96a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\n",
    "# _ = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3440aa8d-a18a-404a-92c7-0dc8918e65f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.n_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3afda4e-c39e-42cf-a7c1-bccc09c5cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"tell me a joke\"]\n",
    "device = 'cpu'\n",
    "# device = 'cuda:0'\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding='max_length', max_length=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dab5b51b-9a3e-4a84-aa59-89e4cd3ef4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "586005a0-270c-4f95-9c0c-bac4751bdbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[33332,   503,   258,  9708,     0,     0,     0,     0,     0,     0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = encoded_input['input_ids']\n",
    "mask = encoded_input['attention_mask']\n",
    "prompt = [\"this is a prompt\"]\n",
    "prompt = tokenizer(prompt, return_tensors='pt').to(device)['input_ids']\n",
    "text_zeroed_padding = (text + 1) * mask\n",
    "text_trimmed = text[text_zeroed_padding.nonzero(as_tuple=True)]\n",
    "torch.cat([text_trimmed, prompt[0]])\n",
    "num_cols_all_padding = (text_zeroed_padding.sum(dim=0) == 0).sum().item()\n",
    "print(text_zeroed_padding)\n",
    "num_cols_all_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1291df97-ccbd-4555-a53d-29c673655b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100, 17250,   644,   338,   510]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedback = ['Be funnier']\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "encoded_feedback = tokenizer(text, return_tensors='pt')\n",
    "text_length = encoded_input['input_ids'].shape[-1]\n",
    "both = torch.cat([encoded_input['input_ids'], encoded_feedback['input_ids']], dim=1)\n",
    "labels = both.clone()\n",
    "labels[:, torch.arange(text_length)] = -100\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f08a1a86-7157-4bdc-b488-fb44fedc4fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17250,   644,   338,   510, 17250,   644,   338,   510]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dedadbe7-4703-4f21-8cd0-b5e45da740ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0717, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(both, labels=labels)\n",
    "output['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb6004ce-160e-4f58-9465-19d2c4cd60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = encoded_input['input_ids'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e2e1f51-f0aa-47f0-86d3-62422cc7af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi (\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**encoded_input, max_length=seq_len+1, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "732a29ca-d184-4cbc-b99d-4d7dfd3e823e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17250,   357]], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "20ba19c3-52b6-4053-a5f4-b13b900bf333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 17, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 18, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 19, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 20, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n",
      "Input length of input_ids is 21, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi (Faster than 100k)\n",
      "\n",
      "Downloads: 11,842\n",
      "\n",
      "Hi (Faster than 100k)\n",
      "\n",
      "Downloads: 11,842\n",
      "\n",
      "\n",
      "Hi (Faster than 100k)\n",
      "\n",
      "Downloads: 11,842\n",
      "\n",
      "Total\n",
      "Hi (Faster than 100k)\n",
      "\n",
      "Downloads: 11,842\n",
      "\n",
      "Total downloads\n",
      "Hi (Faster than 100k)\n",
      "\n",
      "Downloads: 11,842\n",
      "\n",
      "Total downloads:\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    outputs = model.generate(outputs, max_length=1, do_sample=True)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fa418-4d1c-439b-88e3-59b8ad50a029",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f903a4-b4a9-4e72-9ee8-edb33e383955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa4f018-8104-44d5-b057-8b5c19f395de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from envs.AdaptEnv import AdaptEnv\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from models.GPT2RewardModel import GPT2RewardModel\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>', device=device)\n",
    "reward_model = GPT2RewardModel(device=device)\n",
    "\n",
    "env = AdaptEnv('Tell me a joke.', 'Be funnier.', 100, reward_model , tokenizer, tokenizer.eos_token_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9d938-d218-4869-8791-10626cb47ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from models.GPT2ActorCriticPolicy import GPT2ActorCriticPolicy\n",
    "\n",
    "model = PPO(GPT2ActorCriticPolicy, env, device=device, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561afa4-a7f8-4943-bc81-b7e81fcc9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a824de20-50e5-4ac1-a871-20468b3eb779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
