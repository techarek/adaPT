{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db44afb6-b77a-42c0-9c7b-b93fd09166f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from models.GPT2ActorCriticPolicy import GPT2ActorCriticPolicy\n",
    "from envs.AdaptEnv import AdaptEnv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc7857e2-39da-4545-97d4-0eafe861f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'algorithm': 'a2c',\n",
      "  'emotion': 'funny',\n",
      "  'feedback': 'Be funnier.',\n",
      "  'lr': 5e-08,\n",
      "  'max_seq_len': 10,\n",
      "  'max_steps': 0,\n",
      "  'model_name': 'DialoGPT-small',\n",
      "  'n_envs': 2,\n",
      "  'out_type': 'joke',\n",
      "  'policy_kwargs': { 'cache_dir': '/om2/user/rogerjin/.cache/transformers',\n",
      "                     'device': 'cuda:0',\n",
      "                     'ortho_init': False,\n",
      "                     'policy_hfid': 'microsoft/DialoGPT-small',\n",
      "                     'value_hfid': 'microsoft/DialoGPT-small'},\n",
      "  'prompt': 'Tell me a joke.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.GPT2ActorCriticNetwork import GPT2ActorCriticNetwork\n",
    "from models.GPT2ActorCriticPolicy import GPT2ActorCriticPolicy\n",
    "from envs.AdaptEnv import AdaptEnv\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from models.GPT2RewardModel import GPT2RewardModel\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from datetime import datetime\n",
    "\n",
    "# device = 'cpu'\n",
    "device = 'cuda:0'\n",
    "\n",
    "cache_dir = '/om2/user/rogerjin/.cache/transformers'\n",
    "\n",
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "hf_models = {\n",
    "    'gpt2': 'gpt2',\n",
    "    'gpt2-medium': 'gpt2-medium',\n",
    "    'gpt2-large': 'gpt2-large',\n",
    "    'DialoGPT-small': 'microsoft/DialoGPT-small',\n",
    "    'DialoGPT-medium': 'microsoft/DialoGPT-medium',\n",
    "    'DialoGPT-large': 'microsoft/DialoGPT-large'\n",
    "}\n",
    "\n",
    "learning_algos = {\n",
    "    'ppo': PPO,\n",
    "    'a2c': A2C\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'out_type': 'joke',\n",
    "    'prompt': \"Tell me a joke.\",\n",
    "    'feedback': 'Be funnier.',\n",
    "    'emotion': 'funny',\n",
    "    'model_name': 'DialoGPT-small',\n",
    "    'algorithm': 'ppo',\n",
    "    'max_seq_len': 35,\n",
    "    'max_steps': 10000,\n",
    "    'lr': 5e-8, # from https://github.com/openai/lm-human-preferences\n",
    "    'n_envs': 20,\n",
    "}\n",
    "\n",
    "config['policy_kwargs'] = {\n",
    "    'ortho_init': False, # important\n",
    "    'policy_hfid': hf_models[config['model_name']],\n",
    "    'value_hfid': hf_models[config['model_name']],\n",
    "    'device': device,\n",
    "    'cache_dir': cache_dir,\n",
    "}\n",
    "\n",
    "TOY = False\n",
    "TOY = True\n",
    "if TOY:\n",
    "    config['max_steps'] = 0\n",
    "    config['max_seq_len'] = 10\n",
    "    config['algorithm'] = 'a2c'\n",
    "    config['n_envs'] = 2\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "curr_time = datetime.now().strftime('%H-%M-%S')\n",
    "\n",
    "pprint.pprint(config, indent=2)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', pad_token='<|pad|>', device=device, cache_dir=cache_dir)\n",
    "prompt_len = tokenizer(config['prompt'], return_tensors=\"pt\")['input_ids'].shape[-1]\n",
    "\n",
    "reward_model = GPT2RewardModel(device=device) # todo: try out dialogpt?\n",
    "\n",
    "env_kwargs = {\n",
    "    'prompt': config['prompt'],\n",
    "    'feedback': config['feedback'],\n",
    "    'max_seq_len': config['max_seq_len'],\n",
    "    'reward_model': reward_model,\n",
    "    'tokenizer': tokenizer,\n",
    "    'eos_token_id': tokenizer.eos_token_id,\n",
    "    'device': 'cpu' # i think breaks on cuda rn\n",
    "}\n",
    "if config['n_envs'] == 1:\n",
    "    env = AdaptEnv(**env_kwargs)\n",
    "else:\n",
    "    env = make_vec_env(AdaptEnv, n_envs=config['n_envs'], env_kwargs=env_kwargs)\n",
    "\n",
    "learning_algo = learning_algos[config['algorithm']] # default PPO. If TOY, then A2C.\n",
    "\n",
    "n_steps = config['max_seq_len'] - prompt_len\n",
    "model = learning_algo(GPT2ActorCriticPolicy, env, n_steps=n_steps, learning_rate=config['lr'], policy_kwargs=config['policy_kwargs'], device=device, verbose=2)\n",
    "model.policy._modules['action_net'] = GPT2LMHeadModel.from_pretrained(hf_models[config['model_name']]).to(device).lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38458947-c343-4e89-aad4-8dd022040bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = model.policy._modules['mlp_extractor'].policy_latent.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7188c-d152-4ad7-b085-9108094ff59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(config['max_steps'], verbose=2)\n",
    "model.policy._modules['action_net'].bias = torch.nn.Parameter(torch.Tensor([0]*50257).to(device))\n",
    "model = model.policy._modules['mlp_extractor'].policy_latent.lm # todo: figure out if the weights are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7181c4e7-8797-4e40-8d2c-b98c6173693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Tell me a joke.'\n",
    "\n",
    "# out_dir = f'output/jokes/knock-knock/'\n",
    "# os.makedirs(out_dir, exist_ok=True)\n",
    "# with open(f'{out_dir}/prompt.txt', 'w+') as prompt_file:\n",
    "#     prompt_file.write(prompt)\n",
    "\n",
    "cache_dir = '/om2/user/rogerjin/.cache/transformers'\n",
    "hf_id = 'microsoft/DialoGPT-small'\n",
    "# hf_id = 'gpt2-large'\n",
    "device = 'cuda:0'\n",
    "# save_path = '/om2/user/rogerjin/6.884/adaPT/checkpoints/test/DialoGPT-small_ppo_2_lr=5e-06.pt'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(hf_id, cache_dir=cache_dir, device=device)\n",
    "\n",
    "control = False\n",
    "run_name = 'be_funnier'\n",
    "\n",
    "# if control:\n",
    "#     model = GPT2LMHeadModel.from_pretrained(hf_id, cache_dir=cache_dir).to(device)\n",
    "#     out_path = f'{out_dir}/control.txt'\n",
    "# else:\n",
    "#     ppo = PPO.load(save_path)\n",
    "#     model = ppo.policy._modules['mlp_extractor'].policy_latent.lm # todo: figure out if the weights are different\n",
    "#     out_path = f'{out_dir}/{run_name}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7058ebfd-99b1-4ace-89fb-4b07c9a50650",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bb42a4c76e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     output_sequences.extend(lm.generate(\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# disable sampling to test if batching affects output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# when generating, we will use the logits of right-most token to predict the next token\n",
    "# so the padding should be on the left\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token # to avoid an error\n",
    "\n",
    "num_sequences = 100\n",
    "sentences = [prompt] * num_sequences\n",
    "batch_size = 20\n",
    "\n",
    "output_sequences = []\n",
    "\n",
    "for i in range(0, num_sequences, batch_size):\n",
    "    batch = sentences[i:i+batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output_sequences.extend(lm.generate(\n",
    "        input_ids=inputs['input_ids'].to(device),\n",
    "        attention_mask=inputs['attention_mask'].to(device),\n",
    "        do_sample=True, # disable sampling to test if batching affects output\n",
    "        min_length=25,\n",
    "        max_length = 60,\n",
    "        temperature=1.5,\n",
    "        top_k=20,\n",
    "        no_repeat_ngram_size=2\n",
    "    ))\n",
    "\n",
    "decodings = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    decoding = tokenizer.decode(output_sequences[i], skip_special_tokens=True)\n",
    "    print(decoding)\n",
    "#     decodings.append(f'<|DECODING|>{decoding}')\n",
    "#     decodings.append(f'{decoding}\\n')\n",
    "    # you can use skip_special_tokens=True in decode() to remove padding token\n",
    "    # but note that it will also remove other special_tokens\n",
    "\n",
    "# with open(out_path, 'w+') as out:\n",
    "#     out.writelines(decodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf63ed-d7fe-4aa9-8165-e2f087479ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
